import random
import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import os
import sys
import PIL
import cv2
import re
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler

# # keras
import tensorflow as tf
import tensorflow.keras.layers as L
from tensorflow.keras.models import Model, Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing import sequence
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
import albumentations

# plt
import matplotlib.pyplot as plt
# —É–≤–µ–ª–∏—á–∏–º —Ä–∞–∑–º–µ—Ä –≥—Ä–∞—Ñ–∏–∫–æ–≤ –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
from pylab import rcParams
rcParams['figure.figsize'] = 10, 5
# –≥—Ä–∞—Ñ–∏–∫–∏ –≤ svg –≤—ã–≥–ª—è–¥—è—Ç –±–æ–ª–µ–µ —á–µ—Ç–∫–∏–º–∏
# %config InlineBackend.figure_format = 'svg' 
# %matplotlib inline

# NLP
from pymystem3 import Mystem
from nltk.corpus import stopwords 
from nltk.tokenize import word_tokenize 

def mape(y_true, y_pred):
    return np.mean(np.abs((y_pred-y_true)/y_true))

# –≤—Å–µ–≥–¥–∞ —Ñ–∏–∫—Å–∏—Ä—É–π—Ç–µ RANDOM_SEED, —á—Ç–æ–±—ã –≤–∞—à–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã –±—ã–ª–∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º—ã!
RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)

gpus = tf.config.experimental.list_physical_devices('GPU')
if gpus:
  try:
    # Currently, memory growth needs to be the same across GPUs
    for gpu in gpus:
      tf.config.experimental.set_memory_growth(gpu, True)
    logical_gpus = tf.config.experimental.list_logical_devices('GPU')
    print(len(gpus), "Physical GPUs,", len(logical_gpus), "Logical GPUs")
  except RuntimeError as e:
    # Memory growth must be set before GPUs have been initialized
    print(e)

DATA_DIR = '~/CarPrice2/input/'
train = pd.read_csv(DATA_DIR + 'train.csv')
test = pd.read_csv(DATA_DIR + 'test.csv')
sample_submission = pd.read_csv(DATA_DIR + 'sample_submission.csv')

# split –¥–∞–Ω–Ω—ã—Ö
data_train, data_test = train_test_split(train, test_size=0.15, shuffle=True, random_state=RANDOM_SEED)

# –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–∞–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –±–µ–∑ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∏.
categorical_features = ['bodyType', 'brand', 'color', 'engineDisplacement', 'enginePower', 'fuelType', 'model_info', 'name',
  'numberOfDoors', 'vehicleTransmission', '–í–ª–∞–¥–µ–ª—å—Ü—ã', '–í–ª–∞–¥–µ–Ω–∏–µ', '–ü–¢–°', '–ü—Ä–∏–≤–æ–¥', '–†—É–ª—å']

# –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏
numerical_features = ['mileage', 'modelDate', 'productionDate']

# –í–ê–ñ–ù–û! –¥—Ä—è –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –æ–±—ä–µ–¥–∏–Ω—è–µ–º —Ç—Ä–µ–π–Ω –∏ —Ç–µ—Å—Ç –≤ –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç
train['sample'] = 1 # –ø–æ–º–µ—á–∞–µ–º –≥–¥–µ —É –Ω–∞—Å —Ç—Ä–µ–π–Ω
test['sample'] = 0 # –ø–æ–º–µ—á–∞–µ–º –≥–¥–µ —É –Ω–∞—Å —Ç–µ—Å—Ç
test['price'] = 0 # –≤ —Ç–µ—Å—Ç–µ —É –Ω–∞—Å –Ω–µ—Ç –∑–Ω–∞—á–µ–Ω–∏—è price, –º—ã –µ–≥–æ –¥–æ–ª–∂–Ω—ã –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç—å, –ø–æ—ç—Ç–æ–º—É –ø–æ–∫–∞ –ø—Ä–æ—Å—Ç–æ –∑–∞–ø–æ–ª–Ω—è–µ–º –Ω—É–ª—è–º–∏

data = test.append(train, sort=False).reset_index(drop=True) # –æ–±—ä–µ–¥–∏–Ω—è–µ–º
print(train.shape, test.shape, data.shape)

def preproc_data(df_input):
    """ includes several functions to pre-process the predictor data."""
    
    df_output = df_input.copy()
    
    # ################### 1. –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ ############################################################## 
    # —É–±–∏—Ä–∞–µ–º –Ω–µ –Ω—É–∂–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏ –ø—Ä–∏–∑–Ω–∞–∫–∏
    df_output.drop(['description','sell_id',], axis = 1, inplace=True)
    
    
    # ################### Numerical Features ############################################################## 
    # –î–∞–ª–µ–µ –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    for column in numerical_features:
        df_output[column].fillna(df_output[column].median(), inplace=True)
    # —Ç—É—Ç –≤–∞—à –∫–æ–¥ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ NAN
    # ....
    
    # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
    scaler = MinMaxScaler()
    for column in numerical_features:
        df_output[column] = scaler.fit_transform(df_output[[column]])[:,0]
    
    
    
    # ################### Categorical Features ############################################################## 
    # Label Encoding
    for column in categorical_features:
        df_output[column] = df_output[column].astype('category').cat.codes
        
    # One-Hot Encoding: –≤ pandas –µ—Å—Ç—å –≥–æ—Ç–æ–≤–∞—è —Ñ—É–Ω–∫—Ü–∏—è - get_dummies.
    df_output = pd.get_dummies(df_output, columns=categorical_features, dummy_na=False)
    # —Ç—É—Ç –≤–∞—à –∫–æ–¥ –Ω–µ Encoding —Ñ–∏—Ç—á–µ–π
    # ....
    
    
    # ################### Feature Engineering ####################################################
    # —Ç—É—Ç –≤–∞—à –∫–æ–¥ –Ω–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏—é –Ω–æ–≤—ã—Ö —Ñ–∏—Ç—á–µ–π
    # ....
    
    
    # ################### Clean #################################################### 
    # —É–±–∏—Ä–∞–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ –∫–æ—Ç–æ—Ä—ã–µ –µ—â–µ –Ω–µ —É—Å–ø–µ–ª–∏ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å, 
    df_output.drop(['vehicleConfiguration'], axis = 1, inplace=True)
    
    return df_output


df_preproc = preproc_data(data)

    # –¢–µ–ø–µ—Ä—å –≤—ã–¥–µ–ª–∏–º —Ç–µ—Å—Ç–æ–≤—É—é —á–∞—Å—Ç—å
train_data = df_preproc.query('sample == 1').drop(['sample'], axis=1)
test_data = df_preproc.query('sample == 0').drop(['sample'], axis=1)

y = train_data.price.values     # –Ω–∞—à —Ç–∞—Ä–≥–µ—Ç
X = train_data.drop(['price'], axis=1)
X_sub = test_data.drop(['price'], axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, shuffle=True, random_state=RANDOM_SEED)


# NLP Processing

stop_words = set(stopwords.words('russian'))
sw1 = set(", . ' : ; ! ? ‚Ññ % * ( ) [ ] { | } # $ ^ & - + < = > ` ~ 1 2 3 4 5 6 7 8 9 0 | @ ¬∑ \' - ` , -  ‚Äï ")
sw2 = set("¬∑ ‚Ä¢ ‚Äî ‚ùóÔ∏è ‚ú™ \\ / üòÅ üòä üòâ ‚àô ‚úî ‚ñ∫ ‚ÇΩ ‚Ä≥ ¬´ ¬ª ‚Ä¶ ‚úÖ ‚òëÔ∏è ü§¶ ‚óè üî∞ ¬∞ üìå üì¢ ‚òé ‚ñº ‚û• ‚òõ „ÄÇ üîù ‚¨áÔ∏è ‚ñ∂")
stop_words = stop_words.union(sw1)
stop_words = stop_words.union(sw2)
m = Mystem()

def remove_sw_lemma(data, stop_words=stop_words, ms=m):
  words = word_tokenize(data)
  wordsFiltered = ''
  for w in words:
    if w not in stop_words:
      wordsFiltered += w+' '
  return [x for x in ms.lemmatize(wordsFiltered) if x != ' ']

data.description = data.description.apply(remove_sw_lemma)

# TOKENIZER
# The maximum number of words to be used. (most frequent)
MAX_WORDS = 50000
# Max number of words in each complaint.
MAX_SEQUENCE_LENGTH = 256

# split –¥–∞–Ω–Ω—ã—Ö
text_train = data.description.iloc[X_train.index]
text_test = data.description.iloc[X_test.index]
text_sub = data.description.iloc[X_sub.index]

tokenize = Tokenizer(num_words=MAX_WORDS)
tokenize.fit_on_texts(data.description)

text_train_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_train), maxlen=MAX_SEQUENCE_LENGTH)
text_test_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_test), maxlen=MAX_SEQUENCE_LENGTH)
text_sub_sequences = sequence.pad_sequences(tokenize.texts_to_sequences(text_sub), maxlen=MAX_SEQUENCE_LENGTH)

print(text_train_sequences.shape, text_test_sequences.shape, text_sub_sequences.shape, )

# –≤–æ—Ç —Ç–∞–∫ —Ç–µ–ø–µ—Ä—å –≤—ã–≥–ª—è–¥–∏—Ç –Ω–∞—à —Ç–µ–∫—Å—Ç
print(text_train.iloc[6])
print(text_train_sequences[6])

model_nlp = Sequential()
model_nlp.add(L.Input(shape=MAX_SEQUENCE_LENGTH, name="seq_description"))
model_nlp.add(L.Embedding(len(tokenize.word_index)+1, MAX_SEQUENCE_LENGTH,))
model_nlp.add(L.LSTM(256, return_sequences=True))
model_nlp.add(L.Dropout(0.5))
model_nlp.add(L.LSTM(128,))
model_nlp.add(L.Dropout(0.25))
model_nlp.add(L.Dense(64, activation="relu"))
model_nlp.add(L.Dropout(0.25))

model_mlp = Sequential()
model_mlp.add(L.Dense(512, input_dim=X_train.shape[1], activation="relu"))
model_mlp.add(L.Dropout(0.5))
model_mlp.add(L.Dense(256, activation="relu"))
model_mlp.add(L.Dropout(0.5))

combinedInput = L.concatenate([model_nlp.output, model_mlp.output])
# being our regression head
head = L.Dense(64, activation="relu")(combinedInput)
head = L.Dense(1, activation="linear")(head)

model = Model(inputs=[model_nlp.input, model_mlp.input], outputs=head)

optimizer = tf.keras.optimizers.Adam(0.01)
model.compile(loss='MAPE',optimizer=optimizer, metrics=['MAPE'])

checkpoint = ModelCheckpoint('/home/alex/CarPrice2/working/best_model.hdf5', monitor=['val_MAPE'], verbose=0, mode='min')
earlystop = EarlyStopping(monitor='val_MAPE', patience=10, restore_best_weights=True,)
callbacks_list = [checkpoint, earlystop]

history = model.fit([text_train_sequences, X_train], y_train,
                    batch_size=512,
                    epochs=500, # —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –º—ã –æ–±—É—á–∞–µ–º –ø–æ–∫–∞ EarlyStopping –Ω–µ –æ—Å—Ç–∞–Ω–æ–≤–∏—Ç –æ–±—É—á–µ–Ω–∏–µ
                    validation_data=([text_test_sequences, X_test], y_test),
                    callbacks=callbacks_list,
                    verbose=1
                   )

plt.title('Loss')
plt.plot(history.history['MAPE'], label='train')
plt.plot(history.history['val_MAPE'], label='test')
plt.show();

model.load_weights('/home/alex/CarPrice2/working/best_model.hdf5')
model.save('/home/alex/CarPrice2/working/nn_mlp_nlp.hdf5')

test_predict_nn2 = model.predict([text_test_sequences, X_test])
print(f"TEST mape: {(mape(y_test, test_predict_nn2[:,0]))*100:0.2f}%")

sub_predict_nn2 = model.predict([text_sub_sequences, X_sub])
sample_submission['price'] = sub_predict_nn2[:,0]
sample_submission.to_csv('/home/alex/CarPrice2/working/nn2_submission.csv', index=False)

'''–ò–¥–µ–∏ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è NLP —á–∞—Å—Ç–∏:

–í—ã–¥–µ–ª–∏—Ç—å –∏–∑ –æ–ø–∏—Å–∞–Ω–∏–π —á–∞—Å—Ç–æ –≤—Å—Ç—Ä–µ—á–∞—é—â–∏–µ—Å—è –±–ª–æ–∫–∏ —Ç–µ–∫—Å—Ç–∞, –∑–∞–º–µ–Ω–∏–≤ –∏—Ö –Ω–∞ –∫–æ–¥–æ–≤—ã–µ —Å–ª–æ–≤–∞ –∏–ª–∏ —É–¥–∞–ª–∏–≤
–°–¥–µ–ª–∞—Ç—å –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É —Ç–µ–∫—Å—Ç–∞, –Ω–∞–ø—Ä–∏–º–µ—Ä, —Å–¥–µ–ª–∞—Ç—å –ª–µ–º–º–∞—Ç–∏–∑–∞—Ü–∏—é - –∞–ª–≥–æ—Ä–∏—Ç–º —Å—Ç–∞–≤—è—â–∏–π –≤—Å–µ —Å–ª–æ–≤–∞ –≤ —Ñ–æ—Ä–º—É –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–≥–ª–∞–≥–æ–ª—ã –≤ –∏–Ω—Ñ–∏–Ω–∏—Ç–∏–≤ –∏ —Ç. –¥.), —á—Ç–æ–±—ã —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä –Ω–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤—ã–≤–∞–ª —Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º—ã —Å–ª–æ–≤–∞ –≤ —Ä–∞–∑–Ω—ã–µ —á–∏—Å–ª–∞ –°—Ç–∞—Ç—å—è –ø–æ —Ç–µ–º–µ: https://habr.com/ru/company/Voximplant/blog/446738/
–ü–æ—Ä–∞–±–æ—Ç–∞—Ç—å –Ω–∞–¥ –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –æ—á–∏—Å—Ç–∫–∏ –∏ –∞—É–≥–º–µ–Ω—Ç–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞'''
